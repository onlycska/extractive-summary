# -*- coding: utf-8 -*-
"""Копия блокнота "GazetaSummarization.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17eeB2DqkTuvvarNNdMC965U_ZZD_E1lc

## Paper
* Dataset for Automatic Summarization of Russian News: https://arxiv.org/abs/2006.11063
* Download dataset: https://github.com/IlyaGusev/gazeta
* Summarization models: https://github.com/IlyaGusev/summarus

## Requirements
"""
import pickle
import re
import sys
import json
import random
import tqdm
import torch
import nltk
import logging
import numpy as np

from sentence_transformers import SentenceTransformer, util

nltk.download('punkt')


def set_global_logging_level(level: int = logging.ERROR, prefixes: list[str] = None):
    """
    Override logging levels of different modules based on their name as a prefix.
    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.

    Args:
        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR
        - prefixes: list of one or more str prefixes to match (e.g. ["transformers", "torch"]). Optional.
          Default is `[""]` to match all active loggers.
          The match is a case-sensitive `module_name.startswith(prefix)`
    """
    if prefixes is None:
        prefixes = ['']
    prefix_re = re.compile(fr'^(?:{ "|".join(prefixes) })')
    for name in logging.root.manager.loggerDict:
        if re.match(prefix_re, name):
            logging.getLogger(name).setLevel(level)


def read_gazeta_records(file_name: str, shuffle: bool = False, sort_by_date: bool = True, is_test_mode: bool = False):
    assert shuffle != sort_by_date
    records = []
    with open(file_name, 'r', encoding='utf-8') as r:
        for i, line in enumerate(r):
            records.append(json.loads(line))
            if is_test_mode and i == 500:
                break
    if sort_by_date:
        records.sort(key=lambda x: x['date'])
    if shuffle:
        random.shuffle(records)
    return records


def get_embeddings(sentence: str, sentence_model: SentenceTransformer):
    """
    get embeddings on sentence
    :param sentence_model: sentence transformer model
    :param sentence: sentence to get embedding from
    :return: embeddings from model
    """
    embeddings = sentence_model.encode(sentence)
    return embeddings


class SentenceTokenizer:
    last_tokenized_text: list[str] = ['']

    @staticmethod
    def sent_tokenizer(text: str) -> list[str]:
        SentenceTokenizer.last_tokenized_text = nltk.sent_tokenize(text)
        return SentenceTokenizer.last_tokenized_text

    @staticmethod
    def split_text(record: dict[str, str], key='text'):
        """
        Split record[key] on sentences
        :param record: dict with text
        :param key: key in which texthas placed
        :return: generator on sentences
        """
        for sentence in SentenceTokenizer.sent_tokenizer(record[key]):
            yield sentence


def find_similar_sentences(dataset, sentence_model) -> list[dict]:
    new_dataset = []
    with tqdm.tqdm(total=len(dataset), file=sys.stdout) as pbar:
        for i, article in enumerate(dataset):
            # new record for new_dataset
            new_record: dict = {}
            # embeddings for all sentences in article['text']
            text_sentences_embeddings = []
            # embeddings for all sentences in article['summary']
            summary_sentences_embeddings: list = []
            # ids of sentences from article['text'] which are most similar to sentences from article['summary']
            summarized_sentences_ids: set[int] = set()

            # skip empty articles
            if article['text'] is None:
                continue

            # get embeddings for all sentences in article['text']
            for sentence in SentenceTokenizer.split_text(article, 'text'):
                sentence_embedding = get_embeddings(sentence, sentence_model)
                text_sentences_embeddings.append(sentence_embedding)
            tokenized_text = SentenceTokenizer.last_tokenized_text

            # get embeddings for all sentences in article['summary']
            for sentence in SentenceTokenizer.split_text(article, 'summary'):
                sentence_embedding = get_embeddings(sentence, sentence_model)
                summary_sentences_embeddings.append(sentence_embedding)
            text_sentences_embeddings = torch.FloatTensor(np.array(text_sentences_embeddings))

            # find sentences from text which are most similar to summarized sentences
            for sent_emb in summary_sentences_embeddings:
                sent_emb = torch.FloatTensor(sent_emb)
                cos_scores = util.pytorch_cos_sim(text_sentences_embeddings, sent_emb).numpy()
                summarized_sentences_ids.add(np.argmax(cos_scores, axis=0)[0])

            summarized_ids: list[int] = [0] * len(tokenized_text)
            summarized_text = []
            for sentence_id in summarized_sentences_ids:
                summarized_ids[sentence_id] = 1
                summarized_text.append(tokenized_text[sentence_id])
            assert len(tokenized_text) == text_sentences_embeddings.size(dim=0), 'number of sentences and number of ' \
                                                                                 'embeddings of sentences aren\'t ' \
                                                                                 f'equal tokenzied text:\n' \
                                                                                 f'{tokenized_text}\n\nemb size:\n' \
                                                                                 f'{text_sentences_embeddings.size()}'

            new_record['text'] = article['text']
            new_record['summarized_text'] = ' '.join([x for x in summarized_text])
            new_record['summarized_ids'] = summarized_ids
            new_record['sentences_embeddings'] = text_sentences_embeddings
            new_dataset.append(new_record)
            pbar.set_description('processed: %d' % (1 + i))
            pbar.update(1)
    return new_dataset


def json_writer(records: list[dict], filename: str) -> None:
    jsonn = json.dumps(records, ensure_ascii=False)
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(jsonn)


def pickle_writer(records: list[dict], filename: str) -> None:
    with open(filename, 'wb') as f:
        pickle.dump(records, f)


if __name__ == '__main__':
    # turning off all warnings
    set_global_logging_level(logging.ERROR, ['transformers', 'nlp', 'torch', 'tensorflow', 'tensorboard', 'wandb'])
    device = "cuda" if torch.cuda.is_available() else "cpu"

    test_mode = False
    rubert_sentence_model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence').to(device)

    train_records = read_gazeta_records('data/gazeta_train.jsonl', is_test_mode=test_mode)
    train_records = find_similar_sentences(train_records, rubert_sentence_model)
    pickle_writer(train_records, 'data/train_cls.pkl')
    train_records = None

    val_records = read_gazeta_records('data/gazeta_val.jsonl', is_test_mode=test_mode)
    val_records = find_similar_sentences(val_records, rubert_sentence_model)
    pickle_writer(val_records, 'data/val_cls.pkl')
    val_records = None

    test_records = read_gazeta_records('data/gazeta_test.jsonl', is_test_mode=test_mode)
    test_records = find_similar_sentences(test_records, rubert_sentence_model)
    pickle_writer(test_records, 'data/test_cls.pkl')
    test_records = None
