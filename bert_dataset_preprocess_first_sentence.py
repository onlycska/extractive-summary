# -*- coding: utf-8 -*-
'''Копия блокнота 'GazetaSummarization.ipynb'

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17eeB2DqkTuvvarNNdMC965U_ZZD_E1lc

## Paper
* Dataset for Automatic Summarization of Russian News: https://arxiv.org/abs/2006.11063
* Download dataset: https://github.com/IlyaGusev/gazeta
* Summarization models: https://github.com/IlyaGusev/summarus

## Requirements
'''
import re
import sys
import json
import random
import tqdm
import torch
import nltk
import logging
import numpy as np

from more_itertools import locate
from sentence_transformers import SentenceTransformer, util
from transformers import BertTokenizer

nltk.download('punkt')


def set_global_logging_level(level=logging.ERROR, prefixes=None):
    """
    Override logging levels of different modules based on their name as a prefix.
    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.

    Args:
        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR
        - prefixes: list of one or more str prefixes to match (e.g. ['transformers', 'torch']). Optional.
          Default is `['']` to match all active loggers.
          The match is a case-sensitive `module_name.startswith(prefix)`
    """
    if prefixes is None:
        prefixes = ['']
    prefix_re = re.compile(fr"^(?:{ '|'.join(prefixes) })")
    for name in logging.root.manager.loggerDict:
        if re.match(prefix_re, name):
            logging.getLogger(name).setLevel(level)


def read_gazeta_records(file_name: str, shuffle: bool = False, sort_by_date: bool = True):
    assert shuffle != sort_by_date
    records = []
    with open(file_name, 'r', encoding='utf-8') as r:
        for line in r:
            records.append(json.loads(line))
    if sort_by_date:
        records.sort(key=lambda x: x['date'])
    if shuffle:
        random.shuffle(records)
    return records


def cut_text(article: str):
    tokenized_data = tokenizer(article)
    token_indexes = tokenized_data['input_ids']
    tokens = tokenizer.convert_ids_to_tokens(token_indexes)
    tokens, token_indexes = tokens[:410], token_indexes[:410]
    indexes = list(locate(token_indexes, lambda x: x == 119))
    try:
        tokens, token_indexes = tokens[1:indexes[-1] + 1], token_indexes[1:indexes[-1] + 1]
    except IndexError:
        print(IndexError)
        return None
    sentence = tokenizer.convert_tokens_to_string(tokens)
    return sentence


def get_embeddings(sentence: str, sentence_model: SentenceTransformer):
    """
    get embeddings on sentence
    :param sentence_model: sentence transformer model
    :param sentence: sentence to get embedding from
    :return: embeddings from model
    """
    embeddings = sentence_model.encode(sentence)
    return embeddings


def split_text(record, key='text'):
    """
    Split record[key] on sentences
    :param record: dict with text
    :param key: key in which texthas placed
    :return: generator on sentences
    """
    for sentence in nltk.sent_tokenize(record[key]):
        yield sentence


def find_similar_sentences(dataset, sentence_model) -> list[dict]:
    new_dataset = []
    with tqdm.tqdm(total=len(dataset), file=sys.stdout) as pbar:
        for i, article in enumerate(dataset):
            new_record = {}
            text_embs = []
            summ_embs = []
            new_text = cut_text(article['text'])
            if new_text is None:
                continue
            article['text'] = new_text
            for sentence in split_text(article, 'text'):
                embs = get_embeddings(sentence, sentence_model)
                text_embs.append(embs)
            for sentence in split_text(article, 'summary'):
                embs = get_embeddings(sentence, sentence_model)
                summ_embs.append(embs)
                break
            text_embs = torch.FloatTensor(text_embs)
            summ_embs = torch.FloatTensor(summ_embs)
            cos_scores = util.pytorch_cos_sim(text_embs, summ_embs).numpy()
            max = bool(random.getrandbits(1))
            if max:
                scores = np.argmax(cos_scores, axis=0)
            else:
                scores = np.argmin(cos_scores, axis=0)
            tokenized_text = nltk.sent_tokenize(article['text'])

            text = [tokenized_text[scores[0]] + ' [SEP]']
            text = text + [str(sentence + ' [SEP]') for i, sentence in enumerate(tokenized_text)]
            new_record['text'] = ' '.join([x for x in text])
            new_record['label'] = float(max)
            new_dataset.append(new_record)
            pbar.set_description('processed: %d' % (1 + i))
            pbar.update(1)
    return new_dataset


def json_writer(records: list[dict], filename: str) -> None:
    jsonn = json.dumps(records, ensure_ascii=False)
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(jsonn)


if __name__ == '__main__':
    # turning off all warnings
    set_global_logging_level(logging.ERROR, ['transformers', 'nlp', 'torch', 'tensorflow', 'tensorboard', 'wandb'])

    train_records = read_gazeta_records('data/gazeta_train.jsonl')
    val_records = read_gazeta_records('data/gazeta_val.jsonl')
    test_records = read_gazeta_records('data/gazeta_test.jsonl')

    use_model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

    val_records = find_similar_sentences(val_records, use_model)
    json_writer(val_records, 'data/val.json')

    train_records = find_similar_sentences(train_records, use_model)
    json_writer(train_records, 'data/train.json')

    test_records = find_similar_sentences(test_records, use_model)
    json_writer(test_records, 'data/test.json')
